{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The goal of this tutorial is to walk you through some of the core modules used in Python for data analysis.  We're going to run through a simple example to get everyone on the same page.\n",
    "\n",
    "## Jupyter\n",
    "\n",
    "This document is a Jupyter Notebook, a tool for interactively running code interspersed with text and output.  You can create notebooks in a number of different programming languages like Python, R, or Julia.  Let's take a basic example using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can modify the print statement above and rerun the corresponding \"cell\" to have it print whatever you want.\n",
    "\n",
    "Jupyter notebooks are great for classes like this because you can run through the examples on your own machine with me live.\n",
    "\n",
    "Before we get started, we're going to need to import a series of modules or libraries that will be used throughout the rest of the tutorial.  Modules are collections of pre-defined Python functions (and other objects) which you can use in your scripts.  Whereas libraries are larger collections of modules -- we'll mostly be dealing with libraries here though the distinction isn't important.\n",
    "\n",
    "We import libraries as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to import particular functions from a given module, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "X = np.random.normal(size=(10, 10))\n",
    "norm(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "To start, we're going to spend most of our time using the module pandas.  Pandas is a data analysis library that contains many of the tools you'll want to use to work with data in Python.  Let's start by working with some real data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "A dataframe is one of the core objects used in pandas.  It is essentially a matrix with additional metadata associated with the rows and columns.  For instance, the dataframe might have an index which corresponds to days of the week, while each column corresponds to a different assets returns.\n",
    "\n",
    "One great thing about pandas is how easy it is to load data into Python.  You can even load data from the web, for instance lets use the following command to load a dataframe I've posted to github:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/lbybee/pytutorial/master/49_ind_portfolios.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I've done is imported a csv table into a dataframe from the internet!  You can do a lot more than this with pandas.  It can work with many data files, I'd encourage you to check out all the IO options here:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html\n",
    "\n",
    "Now, what do we have in terms of data?\n",
    "\n",
    "These are daily returns for a series of industry portfolios taken from Kenneth French's website:\n",
    "\n",
    "https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html\n",
    "\n",
    "The first column is the date and the remaining columns are each portfolios returns.\n",
    "\n",
    "How to I work with this dataframe?\n",
    "\n",
    "I can access each column as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Agric\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a __series__, which is just a column from a dataframe.  I can also look at multiple columns at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Agric\", \"Food\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I specified these columns with a list as opposed to a single string, this returned a sub-matrix corresponding to a sub-dataframe instead of a series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy\n",
    "\n",
    "Pandas is build on top of numpy -- a module I introduced in our previous TA session.\n",
    "\n",
    "numpy is the core library used for linear algebra in Python and has many tools you'll end up using throughout the class.\n",
    "\n",
    "The dataframe above is really just a wrapper around numpy, and I can access the raw values if I want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Agric\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can do many linear algebra operations on the dataframe itself, and these will behave you would expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Agric\", \"Food\", \"Soda\", \"Beer\"]].T.dot(df[\"Agric\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I've done here is taken the dot product between a series of columns, \"Agric\", \"Food\", \"Soda\", and \"Beer\", with the column \"Agric\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Operations\n",
    "\n",
    "I can do many standard transformations to my dataframe and they'll behave as you expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(df[[\"Agric\", \"Food\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases the operations are contained within the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Agric\"]].cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates\n",
    "\n",
    "Our date column behaves differently from the other columns, how should we treat this separately?\n",
    "\n",
    "First, we can convert the date into a datetime which will allow us to perform operations which make assumptions based on date (we'll see more on these later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y%m%d\")\n",
    "df[\"date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now access various datetime information from our date variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore more on this here:\n",
    "\n",
    "https://pandas.pydata.org/docs/user_guide/timeseries.html\n",
    "\n",
    "I'd recommend checking this out if you intend to use Python long-term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Index\n",
    "\n",
    "If I print my dataframe again, I will see a \"column\" on the left corresponding to a series of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corresponds to the index for my dataframe.  I'm not going to spend too much time on indexes here, they can allow you to do some cool stuff, but one thing we may want to do here is set our date as the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdt = df.set_index(\"date\")\n",
    "dfdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all my columns should have the same type, and I can perform some dataframe wide operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdt.T.dot(dfdt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can often handle much of the indexing and date work when you initially load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdt = pd.read_csv(\"https://raw.githubusercontent.com/lbybee/pytutorial/master/49_ind_portfolios.csv\",\n",
    "                   index_col=\"date\", parse_dates=True)\n",
    "dfdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "Dataframes allow for many ways to access subsets of the data.  For instance, let's say I want to only look at the returns in the first month of 2020:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdt[dfdt.index.month == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, maybe I want to access all the rows where \"Agric\" returns are positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdt[dfdt[\"Agric\"] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is referred to as boolean indexing, because the `dfdt[\"Agric\"] > 0` term yields a series of truth values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdt[\"Agric\"] > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can also do this using multiple columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdt[(dfdt[\"Agric\"] > 0) & (dfdt[\"Food\"] > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns all the rows where both \"Agric\" and \"Food\" returns are positive.  The `&` is used for \"and\" and if I want do \"or\", I can use `|`.\n",
    "\n",
    "I can also index as we did with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdt[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdt[9:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in checking out more about indexing I'd recommend the pandas wiki page:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries and Aggregates\n",
    "\n",
    "Now that we've explored how to manipulate the dataframe, let's put together some summaries of the data.\n",
    "\n",
    "A basic command that you can run to get a good sense of your data is `describe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can also run many other standard operations here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdt[[\"Agric\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdt[[\"Agric\"]].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdt[[\"Agric\", \"Food\"]].aggregate([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby\n",
    "\n",
    "A very useful tool to understand in pandas is groupby.  Groupby is a way to apply operations to a subsets of your data in a systematic way.  For instance, what if I want to get the mean return for each asset for each month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdt.groupby(pd.Grouper(freq=\"M\")).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a datetime index, I can group by this `pd.Grouper(freq=\"M\")` object.  The `freq=\"M\"` specifies what date frequency I want to use (in this case months), but I could specify other options, e.g. years or `freq=\"Y\"`.\n",
    "\n",
    "I can also groupby columns.  Let's assume we have another column corresponding to indicators for whether or not there is an FOMC meeting that day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdt[\"FOMC\"] = np.random.randint(0, 2, dfdt.shape[0])\n",
    "dfdt.groupby(\"FOMC\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can also groupby both values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdt.groupby([pd.Grouper(freq=\"Q\"), \"FOMC\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives me the mean for each quarter/FOMC meeting pair.\n",
    "\n",
    "Sometimes I may want to perform a groupby operation and update the original dataframe.  For instance, perhaps I want to subtract the monthly mean from each return series.  I can do this with the `transform` operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdt.groupby(pd.Grouper(freq=\"M\")).transform(lambda x: x - x.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a dataframe of the same shape as the input.  The `lambda x: x - x.mean()` is a method for quickly defining functions inline -- in this case a function to demean.\n",
    "\n",
    "Groupby is extremely powerful, I've only scratched the surface here, I'd encourage you to check out the corresponding wiki for more:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging\n",
    "\n",
    "I may at some point have multiple dataframes loaded and want to merge them.  For instance, perhaps I have a number of possible predictor variables for returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffdf = pd.read_csv(\"https://raw.githubusercontent.com/lbybee/pytutorial/master/FF3.csv\",\n",
    "                   index_col=\"date\", parse_dates=True)\n",
    "dfdt = dfdt.drop([\"FOMC\"], axis=1, errors=\"ignore\")\n",
    "ffdf = dfdt.merge(ffdf, right_index=True, left_index=True)\n",
    "print(ffdf.columns)\n",
    "ffdf[[\"MktmRF\", \"SMB\", \"HML\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging can get complex, here it is simple because of how the indices are defined but I'd recommend reading the wiki and verifying that the merge does what you expect (by examining the data) when you start doing this:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matplotlib\n",
    "\n",
    "So far we've put together some basic summaries of our data and explored how to manipulate dataframes.  However, often the best way to get a sense for a new data set is to draw some plots.\n",
    "\n",
    "To do this we'll use the matplotlib library imported above:\n",
    "\n",
    "https://matplotlib.org/\n",
    "\n",
    "There are many other cool libraries available that I'd encourage you to check out as well, e.g. seaborn:\n",
    "\n",
    "https://seaborn.pydata.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Plots\n",
    "\n",
    "Let's start by generating some basic time series plots to see how our returns behave.  I can start by plotting the cumulative returns for `\"Agric\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ffdf[\"Agric\"].cumsum())\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative Return\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I want to plot multiple return series alongside each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ffdf[\"Agric\"].cumsum(), label=\"Agric\")\n",
    "plt.plot(ffdf[\"Food\"].cumsum(), label=\"Food\")\n",
    "plt.plot(ffdf[\"Autos\"].cumsum(), label=\"Autos\")\n",
    "plt.plot(ffdf[\"Banks\"].cumsum(), label=\"Banks\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to specify the column because our index is a date.  I could alternatively, tell matplotlib the `x` and `y` values separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ffdf.index, ffdf[\"Agric\"].cumsum().values)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative Return\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plots\n",
    "\n",
    "Does \"Agric\" have any market beta?  Let's look at a scatter plot to get a sense of the correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ffdf[\"MktmRF\"], ffdf[\"Agric\"])\n",
    "plt.xlabel(\"MktmRF\")\n",
    "plt.ylabel(\"Agric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build-In Pandas Plotting\n",
    "\n",
    "Pandas can also create a number of different plots on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffdf[[\"Agric\", \"Food\", \"Autos\", \"Banks\"]].boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffdf[[\"Agric\", \"Food\", \"Autos\", \"Banks\"]].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(ffdf[[\"Agric\", \"Food\", \"Autos\", \"Banks\", \"MktmRF\"]], diagonal=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full list of the pandas in-house plotting options are available here:\n",
    "\n",
    "\n",
    "https://pandas.pydata.org/docs/user_guide/visualization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps\n",
    "\n",
    "Matplotlib is a fairly established library with many useful tools and tricks.  You can spend a considerable amount of time refining your plots to get exactly what you want.  Let me show you one last cool example before we move forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = dfdt.cov()\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "vlim = np.max([np.abs(np.min(cov)), np.max(cov)])\n",
    "heatmap = ax.pcolor(cov.values, cmap=plt.cm.seismic, vmin=-vlim, vmax=vlim)\n",
    "ax.set_xticks(np.arange(cov.shape[1]))\n",
    "ax.set_xticklabels(cov.columns, rotation=90, fontsize=\"small\")\n",
    "ax.set_yticks(np.arange(cov.shape[1]))\n",
    "ax.set_yticklabels(cov.index, fontsize=\"small\")\n",
    "plt.colorbar(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statsmodels\n",
    "\n",
    "While plots and descriptive summaries are nice, we often want fuller statistical models to understand asset prices.  There are a number of useful statistical libraries available in Python.  I'm just going to touch on two here and introduce you to methods for accessing more.\n",
    "\n",
    "The first of these is statsmodels:\n",
    "\n",
    "https://www.statsmodels.org/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Let's fit a regression of our predictor variables on one of our return series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "mod = smf.ols(\"Agric ~ MktmRF + HML + SMB\", data=ffdf)\n",
    "fit = mod.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn\n",
    "\n",
    "Statsmodels has many rigorous statistical methods, however for more \"machine learning\" applications, I'd recommend sklearn as a first stop:\n",
    "\n",
    "https://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso/Penalized Models\n",
    "\n",
    "The lasso is a useful tool for high-dimensional data sets.  If I have a large number of possible predictors, many of which have no association with my outcome variable, I can run a lasso to perform selection.  I won't go into the details of how this works but just show an example with our data above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "noise = pd.DataFrame(np.random.normal(scale=0.1, size=(ffdf.shape[0], 100)), index=ffdf.index)\n",
    "ldf = ffdf[[\"Agric\", \"MktmRF\", \"HML\", \"SMB\"]].merge(noise, right_index=True, left_index=True)\n",
    "mod = linear_model.Lasso(alpha=0.00075)\n",
    "mod.fit(ldf.drop([\"Agric\"], axis=1), ldf[\"Agric\"])\n",
    "coef = pd.Series(mod.coef_, index=[c for c in ldf.columns if c != \"Agric\"])\n",
    "coef[coef != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Libraries and Beyond\n",
    "\n",
    "I've only touched on the very basics of what's possible in Python.  There are many other specialized libraries out there containing various useful functions.  As you explore these you'll want to get familiar with your package manager -- a tool for installing modules/libraries.\n",
    "\n",
    "If you used Anaconda, you should be able to install packages by running:\n",
    "\n",
    "`conda install <package>`\n",
    "\n",
    "where `<package>` is the name of the library.  Otherwise one of the standard package manager in Python is pip:\n",
    "\n",
    "`pip install <package>`.\n",
    "\n",
    "In either case you can check out the document for more on this here:\n",
    "\n",
    "https://docs.anaconda.com/anaconda/user-guide/tasks/install-packages/\n",
    "\n",
    "https://pip.pypa.io/en/stable/reference/pip_install/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
