{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e88cad-a618-46b7-a8f3-0536fbed91b2",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90313851-3051-4ddb-8e4f-ac68efae6a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cf102f-b7a6-4bc0-a877-33d766d22429",
   "metadata": {},
   "source": [
    "## Hashing Vectorizer (No need for explicit vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3695c7-ee60-4de5-8abf-21dc547002ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=10)\n",
    "hv.transform(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81550f-618a-47a2-930f-ec08f4d7d61c",
   "metadata": {},
   "source": [
    "HashingVectorizer and TfidfVectorizer are both techniques used for text feature extraction in natural language processing, but they operate in fundamentally different ways and have distinct characteristics:\n",
    "\n",
    "## Feature Generation:\n",
    "\n",
    "HashingVectorizer: It uses a hash function to map tokens to a fixed number of features (hashes). The hash values are directly used as feature indices, and each feature corresponds to a hash. This results in a fixed-size feature space regardless of the size of the vocabulary or the dataset.\n",
    "\n",
    "TfidfVectorizer: It constructs a vocabulary of unique words (terms) from the text data and counts the occurrence of each term in each document. Then, it computes the Term Frequency-Inverse Document Frequency (TF-IDF) value for each term-document pair. TF-IDF is a measure that reflects both the importance of a term in a document and its rarity in the entire corpus. The result is a sparse matrix where each row corresponds to a document, and each column corresponds to a unique term in the vocabulary.\n",
    "\n",
    "## Memory Efficiency:\n",
    "\n",
    "HashingVectorizer: It is memory-efficient, especially for large datasets and when the vocabulary size is large. Since it doesn't store a vocabulary explicitly, it has a fixed memory footprint.\n",
    "\n",
    "TfidfVectorizer: It can be memory-intensive because it requires storing the vocabulary and the term-document matrix. The memory usage is directly proportional to the vocabulary size and the number of documents.\n",
    "\n",
    "## Interpretability:\n",
    "\n",
    "HashingVectorizer: It is less interpretable because the mapping from tokens to features is determined by the hash function, and the same token may map to different features in different documents.\n",
    "\n",
    "TfidfVectorizer: It is more interpretable because it explicitly maintains a vocabulary of terms, and each feature corresponds to a specific term. This makes it easier to understand which terms are contributing to the feature vectors.\n",
    "\n",
    "## Scalability:\n",
    "\n",
    "HashingVectorizer: It is highly scalable and suitable for large datasets because the feature space size is fixed.\n",
    "\n",
    "TfidfVectorizer: It can be less scalable when dealing with very large vocabularies because it needs to store and process the entire vocabulary.\n",
    "\n",
    "## Feature Dimension:\n",
    "\n",
    "HashingVectorizer: The feature dimension is fixed and determined by the number of features specified when creating the vectorizer.\n",
    "\n",
    "TfidfVectorizer: The feature dimension varies depending on the size of the vocabulary and the number of unique terms in the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee7a99c-86f8-499e-a457-a0a79ffda332",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3ccb10-7633-4c44-805f-ebc9ed412174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "         early_stopping_rounds=10,\n",
    "         eval_metric=\"logloss\",\n",
    "         eval_set=[(x_eval,y_eval)])\n",
    "\n",
    "y_pred=model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f541951d-72a5-465d-9beb-373f7deb9acb",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f2ac83-ef9a-44ca-abf8-cce98b4140eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrcis.pairwise import cosine_similarity\n",
    "\n",
    "# Between Two Vectors\n",
    "sim = cosine_similarity(x,y)[0,0]\n",
    "\n",
    "# Between all rows of a matrix\n",
    "sim = cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a865a-3e60-4157-bfc0-f11a90786c39",
   "metadata": {},
   "source": [
    "## K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a79c6a-3658-499e-8b56-c1fc19ace12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import Kmeans\n",
    "kmeans = Kmeans(n_clusters=10)\n",
    "kkeans.fit(X)\n",
    "assigned_clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7c0f2-f4cd-48af-99d3-d11c710be85a",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a153977-ddf1-48fd-b5bb-8f969f8e03b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_component=10)\n",
    "X_train_pca = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c20b6-c33e-4acf-8f74-6d664bf5f50b",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de978ca-e830-4b70-a301-cf9ed0b52492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "# Train LDA with 10 topics\n",
    "ldf = LdaModel(doc_term_matrix, num_topic=10,\n",
    "              id2word=dictionary, passes=3)\n",
    "lda.show_topics(formatted=false)\n",
    "\n",
    "# To get topic porpotion to a document, use the correspoding row of the document term matrix\n",
    "lda[doc_term_matrix[1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
